{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AB_lw78VHPvi",
        "Q_RmYQwcAzOc",
        "-UtV7irdR1gO",
        "OPeeF27ZTWye",
        "SRc_3JIJxzY5",
        "pFvn31f7zx3M",
        "kUY7tAF0z9MP",
        "yWdVeJaW0U29",
        "WCKv2Z7R1DjQ",
        "mLGJLfmY1emx",
        "mMJprzi512zy",
        "ipvcCYrF6Rr_",
        "XLUMwPsbnHQ9",
        "9kM3gJh2n4vg",
        "Qob023ajocrD",
        "kBGtoyVGo7K7",
        "T0DMam1Epwjx",
        "T8Jo-UpLswrh",
        "TwvXYF8YtHqi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ef22Mwq69_P"
      },
      "outputs": [],
      "source": [
        "!pip install videohash >> None\n",
        "!pip install opencv-python >> None\n",
        "!pip install line_profiler >> None\n",
        "!pip install memory_profiler >> None\n",
        "!pip install ffmpeg-python >> None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from videohash import VideoHash\n",
        "import time\n",
        "import numpy as np\n",
        "import ffmpeg\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import cv2\n",
        "from sklearn.cluster import KMeans\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import networkx as nx\n",
        "import os\n",
        "import urllib.request\n",
        "from moviepy.editor import VideoFileClip\n",
        "import face_recognition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "import cProfile\n",
        "import profile\n",
        "from memory_profiler import profile"
      ],
      "metadata": {
        "id": "iEwJ7TEK7QA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cProfile.run('my_function()')\n",
        "profile.run('my_function()')\n",
        "timeit.timeit('my_function()', globals=globals(), number=1000)\n",
        "\n",
        "# line profiler\n",
        "@profile\n",
        "def my_function():\n",
        "    # Ваш код\n",
        "\n",
        "my_function()\n",
        "\n",
        "# memory profiler\n",
        "@profile\n",
        "def my_function():\n",
        "    # Ваш код\n",
        "\n",
        "my_function()"
      ],
      "metadata": {
        "id": "yq5i5QbS9o3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Сроавнение по Hash"
      ],
      "metadata": {
        "id": "jW7FRNDxKa7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "url1 = \"https://user-images.githubusercontent.com/64683866/168872267-7c6682f8-7294-4d9a-8a68-8c6f44c06df6.mp4\"\n",
        "url2 = \"https://user-images.githubusercontent.com/64683866/168869109-1f77c839-6912-4e24-8738-42cb15f3ab47.mp4\"\n",
        "#url3 = \"https://user-images.githubusercontent.com/64683866/148960165-a210f2d2-6c41-4349-bd8d-a4cb673bc0af.mp4\"\n",
        "\n",
        "videohash1 = VideoHash(url=url1)\n",
        "videohash2 = VideoHash(url=url2)\n",
        "#videohash3 = VideoHash(url=url3)\n",
        "\n",
        "print(videohash2.is_similar(videohash1))\n",
        "#print(videohash3.is_diffrent(videohash2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klZhne4A7R6i",
        "outputId": "f561b216-762f-4b42-ee70-1193a926f36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
            "Wall time: 9.54 µs\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Снижение разрешения каждого кадра для облегчения обработки"
      ],
      "metadata": {
        "id": "AB_lw78VHPvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_resolution(input_video_path, output_video_path, scale_factor):\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * scale_factor)\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * scale_factor)\n",
        "\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        resized_frame = cv2.resize(frame, (width, height))\n",
        "\n",
        "        out.write(resized_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "nSH8Z9O1LKYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_video = '/content/4.mp4'\n",
        "output_video = '/content/output_1.mp4'"
      ],
      "metadata": {
        "id": "CHu0vXnpLMog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale = 0.2\n",
        "reduce_resolution(input_video, output_video, scale)"
      ],
      "metadata": {
        "id": "0UbpCIukGkbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Выделение ключевых кадров (с целью подрезать дублирующие и не представляющие особой днамики)"
      ],
      "metadata": {
        "id": "Q_RmYQwcAzOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    while success:\n",
        "        frames.append(frame)\n",
        "        success, frame = cap.read()\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "zpQH2Ik4LPpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_duplicate(frame1, frame2, threshold=0.95):\n",
        "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "    score = ssim(gray1, gray2)\n",
        "    return score > threshold"
      ],
      "metadata": {
        "id": "tWQCKcBsLRpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(frames):\n",
        "    unique_frames = []\n",
        "\n",
        "    for i in range(len(frames)):\n",
        "        if i == 0 or not is_duplicate(frames[i], frames[i-1]):\n",
        "            unique_frames.append(frames[i])\n",
        "\n",
        "    return unique_frames"
      ],
      "metadata": {
        "id": "FGT0G-VsLTC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/4.mp4'\n",
        "frames = extract_frames(video_path)\n",
        "unique_frames = remove_duplicates(frames)\n",
        "#for i, frame in enumerate(unique_frames):\n",
        "#    cv2.imwrite(f'unique_frame_{i}.jpg', frame)\n",
        "print(f'Общее количество кадров: {len(frames)}')\n",
        "print(f'Количество уникальных кадров: {len(unique_frames)}')"
      ],
      "metadata": {
        "id": "IN3GchFWLUra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Анализ цветовой палитры видео и сравнение их для выявления схожести"
      ],
      "metadata": {
        "id": "-UtV7irdR1gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    while success:\n",
        "        frames.append(frame)\n",
        "        success, frame = cap.read()\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "SsHRHp_yR_bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_palette(frame, n_colors=5):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pixels = frame.reshape(-1, 3)\n",
        "    kmeans = KMeans(n_clusters=n_colors)\n",
        "    kmeans.fit(pixels)\n",
        "    return kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "Rye-wMGBSBNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_palettes(palette1, palette2):\n",
        "    distance = np.linalg.norm(palette1 - palette2)\n",
        "    return distance"
      ],
      "metadata": {
        "id": "uAZ3v35PSDIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_videos(video_paths):\n",
        "    palettes = {}\n",
        "\n",
        "    for video_path in video_paths:\n",
        "        frames = extract_frames(video_path)\n",
        "        if frames:\n",
        "            palette = extract_palette(frames[0])\n",
        "            palettes[video_path] = palette\n",
        "\n",
        "    similar_videos = []\n",
        "\n",
        "    for i in range(len(video_paths)):\n",
        "        for j in range(i + 1, len(video_paths)):\n",
        "            distance = compare_palettes(palettes[video_paths[i]], palettes[video_paths[j]])\n",
        "            if distance < 500:\n",
        "                similar_videos.append((video_paths[i], video_paths[j]))\n",
        "\n",
        "    return similar_videos"
      ],
      "metadata": {
        "id": "CWEa6XdsSFoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_paths = ['/content/4.mp4', '/content/4_2.mp4']"
      ],
      "metadata": {
        "id": "TH1CQmpkSHse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_videos = find_similar_videos(video_paths)\n",
        "\n",
        "if similar_videos:\n",
        "    print(\"Найденные похожие видео:\")\n",
        "    for video_pair in similar_videos:\n",
        "        print(f\"{video_pair[0]} и {video_pair[1]} похожи.\")\n",
        "else:\n",
        "    print(\"Похожие видео не найдены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFqJ8fw0RzBw",
        "outputId": "30dad9b4-8815-4754-a78a-397c5ee5c005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найденные похожие видео:\n",
            "/content/4.mp4 и /content/4_2.mp4 похожи.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Анализ частотной области (Использование преобразования Фурье для анализа частотных компонентов видео)."
      ],
      "metadata": {
        "id": "OPeeF27ZTWye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    while success:\n",
        "        frames.append(frame)\n",
        "        success, frame = cap.read()\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "d9PQbsptT-Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_frequency(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    f_transform = np.fft.fft2(gray_frame)\n",
        "    f_transform_shifted = np.fft.fftshift(f_transform)\n",
        "    magnitude_spectrum = np.abs(f_transform_shifted)\n",
        "    return magnitude_spectrum"
      ],
      "metadata": {
        "id": "fxN1DqM1UC40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_frequencies(freq1, freq2):\n",
        "    diff = freq1 - freq2\n",
        "    rmse = np.sqrt(np.mean(diff**2))\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "8M7kmBP1UEX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_videos(video_paths):\n",
        "    frequencies = {}\n",
        "\n",
        "    for video_path in video_paths:\n",
        "        frames = extract_frames(video_path)\n",
        "        if frames:\n",
        "            freq = compute_frequency(frames[0])\n",
        "            frequencies[video_path] = freq\n",
        "\n",
        "    similar_videos = []\n",
        "\n",
        "    for i in range(len(video_paths)):\n",
        "        for j in range(i + 1, len(video_paths)):\n",
        "            rmse = compare_frequencies(frequencies[video_paths[i]], frequencies[video_paths[j]])\n",
        "            if rmse < 1000000:\n",
        "                similar_videos.append((video_paths[i], video_paths[j]))\n",
        "\n",
        "    return similar_videos"
      ],
      "metadata": {
        "id": "9XKv0rTtUGUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_paths = [\n",
        "    '/content/4.mp4',\n",
        "    '/content/4_2.mp4']\n",
        "\n",
        "similar_videos = find_similar_videos(video_paths)\n",
        "\n",
        "if similar_videos:\n",
        "    print(\"Найденные похожие видео:\")\n",
        "    for video_pair in similar_videos:\n",
        "        print(f\"{video_pair[0]} и {video_pair[1]} похожи.\")\n",
        "else:\n",
        "    print(\"Похожие видео не найдены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOjJ6Hg3TXJs",
        "outputId": "92462231-b5b8-4a22-e003-f93bee154e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найденные похожие видео:\n",
            "/content/4.mp4 и /content/4_2.mp4 похожи.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Сравнение дубликатов видео по Average Hash (AHash)"
      ],
      "metadata": {
        "id": "SRc_3JIJxzY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_hash(image, hash_size=8):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, (hash_size, hash_size))\n",
        "    avg = np.mean(image)\n",
        "    return ''.join('1' if pixel > avg else '0' for pixel in image.flatten())\n",
        "\n",
        "def process_video_ahash(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    hashes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        ahash = average_hash(frame)\n",
        "        hashes.append(ahash)\n",
        "\n",
        "    cap.release()\n",
        "    return hashes\n",
        "\n",
        "def compare_hashes(hashes1, hashes2):\n",
        "    matches = sum(h1 == h2 for h1, h2 in zip(hashes1, hashes2))\n",
        "    return matches\n",
        "\n",
        "# Пример использования\n",
        "video_path1 = '/content/4_2.mp4'\n",
        "video_path2 = '/content/5.mp4'\n",
        "\n",
        "ahash_values1 = process_video_ahash(video_path1)\n",
        "ahash_values2 = process_video_ahash(video_path2)\n",
        "\n",
        "match_count = compare_hashes(ahash_values1, ahash_values2)\n",
        "print(f\"AHash совпадающих кадров: {match_count}\")"
      ],
      "metadata": {
        "id": "SIT736IWTsW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20748e3-7b93-4090-fb0d-9b87b6d86ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AHash совпадающих кадров: 736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Difference Hash (DHash)"
      ],
      "metadata": {
        "id": "pFvn31f7zx3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def difference_hash(image, hash_size=8):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, (hash_size + 1, hash_size))\n",
        "    diff = image[:, 1:] > image[:, :-1]\n",
        "    return ''.join('1' if x else '0' for x in diff.flatten())\n",
        "\n",
        "def process_video_dhash(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    hashes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        dhash = difference_hash(frame)\n",
        "        hashes.append(dhash)\n",
        "\n",
        "    cap.release()\n",
        "    return hashes\n",
        "\n",
        "video_path1 = '/content/4_2.mp4'\n",
        "video_path2 = '/content/5.mp4'\n",
        "\n",
        "dhash_values1 = process_video_dhash(video_path1)\n",
        "dhash_values2 = process_video_dhash(video_path2)\n",
        "\n",
        "match_count = compare_hashes(dhash_values1, dhash_values2)\n",
        "print(f\"DHash совпадающих кадров: {match_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAcyloucztOF",
        "outputId": "aee871f9-8383-48e5-ec66-4776c300c16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DHash совпадающих кадров: 679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Perceptual Hash (PHash)"
      ],
      "metadata": {
        "id": "kUY7tAF0z9MP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptual_hash(image, hash_size=8):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, (hash_size, hash_size))\n",
        "    dct = cv2.dct(np.float32(image))\n",
        "    dct_low_freq = dct[:hash_size // 2, :hash_size // 2]\n",
        "    avg = np.mean(dct_low_freq)\n",
        "    return ''.join('1' if pixel > avg else '0' for pixel in dct_low_freq.flatten())\n",
        "\n",
        "def process_video_phash(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    hashes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        phash = perceptual_hash(frame)\n",
        "        hashes.append(phash)\n",
        "\n",
        "    cap.release()\n",
        "    return hashes\n",
        "\n",
        "# Пример использования\n",
        "video_path1 = '/content/4_2.mp4'\n",
        "video_path2 = '/content/5.mp4'\n",
        "\n",
        "phash_values1 = process_video_phash(video_path1)\n",
        "phash_values2 = process_video_phash(video_path2)\n",
        "\n",
        "match_count = compare_hashes(phash_values1, phash_values2)\n",
        "print(f\"PHash совпадающих кадров: {match_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk76-Z-tz3qs",
        "outputId": "3d59cfb7-f897-43a9-842d-8b526d436fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PHash совпадающих кадров: 870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Использование графов: Построение графов на основе характеристик видео для выявления дубликатов на основе связей."
      ],
      "metadata": {
        "id": "yWdVeJaW0U29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_hash(image, hash_size=8):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, (hash_size, hash_size))\n",
        "    avg = np.mean(image)\n",
        "    return ''.join('1' if pixel > avg else '0' for pixel in image.flatten())\n",
        "\n",
        "def process_video_ahash(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    hashes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        ahash = average_hash(frame)\n",
        "        hashes.append(ahash)\n",
        "\n",
        "    cap.release()\n",
        "    return hashes\n",
        "\n",
        "def create_graph(videos):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for i, video1 in enumerate(videos):\n",
        "        hashes1 = process_video_ahash(video1)\n",
        "        G.add_node(video1, hashes=hashes1)\n",
        "\n",
        "        for j, video2 in enumerate(videos):\n",
        "            if i >= j:\n",
        "                continue\n",
        "\n",
        "            hashes2 = process_video_ahash(video2)\n",
        "            similarity = sum(h1 == h2 for h1, h2 in zip(hashes1, hashes2))\n",
        "\n",
        "            # Устанавливаем порог для создания ребра\n",
        "            if similarity > 0:  # Можно изменить порог\n",
        "                G.add_edge(video1, video2, weight=similarity)\n",
        "\n",
        "    return G\n",
        "\n",
        "def find_duplicates(G):\n",
        "    components = list(nx.connected_components(G))\n",
        "    return components\n",
        "\n",
        "# Пример использования\n",
        "video_files = ['/content/4_2.mp4', '/content/5.mp4']\n",
        "\n",
        "graph = create_graph(video_files)\n",
        "duplicate_groups = find_duplicates(graph)\n",
        "\n",
        "for group in duplicate_groups:\n",
        "    print(\"Группа дубликатов:\", group)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG3OarUC0Exb",
        "outputId": "93f35aa4-706a-4206-80ee-3e5a9d09563f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Группа дубликатов: {'/content/4_2.mp4', '/content/5.mp4'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Перцептивное хеширование"
      ],
      "metadata": {
        "id": "WCKv2Z7R1DjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptual_hash(image, hash_size=8):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, (hash_size, hash_size))\n",
        "    dct = cv2.dct(np.float32(image))\n",
        "    dct_low_freq = dct[:hash_size // 2, :hash_size // 2]\n",
        "    avg = np.mean(dct_low_freq)\n",
        "    return ''.join('1' if pixel > avg else '0' for pixel in dct_low_freq.flatten())\n",
        "\n",
        "def process_video_phash(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    hashes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        phash = perceptual_hash(frame)\n",
        "        hashes.append(phash)\n",
        "\n",
        "    cap.release()\n",
        "    return hashes\n",
        "\n",
        "def compare_hashes(hashes1, hashes2):\n",
        "    matches = sum(h1 == h2 for h1, h2 in zip(hashes1, hashes2))\n",
        "    return matches\n",
        "\n",
        "video_path1 = '/content/4_2.mp4'\n",
        "video_path2 = '/content/5.mp4'\n",
        "\n",
        "phash_values1 = process_video_phash(video_path1)\n",
        "phash_values2 = process_video_phash(video_path2)\n",
        "\n",
        "match_count = compare_hashes(phash_values1, phash_values2)\n",
        "print(f\"PHash совпадающих кадров: {match_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZWbWBoK0pIz",
        "outputId": "e454c050-f198-4aad-9d41-5be6eab70a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PHash совпадающих кадров: 870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Изучение глубины цвета и цветовых схем"
      ],
      "metadata": {
        "id": "mLGJLfmY1emx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def calculate_color_histogram(frame, bins=256):\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    h_hist = cv2.calcHist([hsv_frame], [0], None, [bins], [0, 180])\n",
        "    s_hist = cv2.calcHist([hsv_frame], [1], None, [bins], [0, 256])\n",
        "    v_hist = cv2.calcHist([hsv_frame], [2], None, [bins], [0, 256])\n",
        "\n",
        "    h_hist = cv2.normalize(h_hist, None).flatten()\n",
        "    s_hist = cv2.normalize(s_hist, None).flatten()\n",
        "    v_hist = cv2.normalize(v_hist, None).flatten()\n",
        "\n",
        "    return h_hist, s_hist, v_hist\n",
        "\n",
        "def compare_histograms(hist1, hist2):\n",
        "    # Используем метод сравнения гистограмм (например, корреляцию)\n",
        "    h_similarity = cv2.compareHist(hist1[0], hist2[0], cv2.HISTCMP_CORREL)\n",
        "    s_similarity = cv2.compareHist(hist1[1], hist2[1], cv2.HISTCMP_CORREL)\n",
        "    v_similarity = cv2.compareHist(hist1[2], hist2[2], cv2.HISTCMP_CORREL)\n",
        "\n",
        "    return (h_similarity + s_similarity + v_similarity) / 3\n",
        "\n",
        "video_path1 = '/content/4_2.mp4'\n",
        "video_path2 = '/content/5.mp4'\n",
        "\n",
        "frames1 = extract_frames(video_path1)\n",
        "frames2 = extract_frames(video_path2)\n",
        "\n",
        "num_frames_to_compare = min(10, len(frames1), len(frames2))\n",
        "\n",
        "similarity_scores = []\n",
        "for i in range(num_frames_to_compare):\n",
        "    hist1 = calculate_color_histogram(frames1[i])\n",
        "    hist2 = calculate_color_histogram(frames2[i])\n",
        "    similarity = compare_histograms(hist1, hist2)\n",
        "    similarity_scores.append(similarity)\n",
        "\n",
        "average_similarity = np.mean(similarity_scores)\n",
        "print(f\"Средняя схожесть между видео: {average_similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oof0UAMD1Lvk",
        "outputId": "07895dea-f0c9-40bc-fd32-ada436e14f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Средняя схожесть между видео: 0.9988098422954149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Определение и анализ объектов на кадрах с целью нахождения дубликатов по содержимому, а не только по визуальному представлению;"
      ],
      "metadata": {
        "id": "mMJprzi512zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, filename):\n",
        "    if not os.path.isfile(filename):\n",
        "        print(f\"Скачивание {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(f\"{filename} скачан.\")\n",
        "    else:\n",
        "        print(f\"{filename} уже существует.\")\n",
        "\n",
        "def download_yolo_files():\n",
        "    base_url = \"https://github.com/AlexeyAB/darknet/blob/master/cfg/\"\n",
        "    cfg_file = \"yolov3.cfg\"\n",
        "    weights_file = \"yolov3.weights\"\n",
        "\n",
        "    download_file(f\"{base_url}{cfg_file}?raw=true\", cfg_file)\n",
        "    download_file(f\"https://pjreddie.com/media/files/{weights_file}\", weights_file)\n",
        "\n",
        "def load_yolo():\n",
        "    download_yolo_files()\n",
        "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
        "    layer_names = net.getLayerNames()\n",
        "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    return net, output_layers\n",
        "\n",
        "def detect_objects(frame, net, output_layers):\n",
        "    height, width = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outputs = net.forward(output_layers)\n",
        "\n",
        "    boxes, confidences, class_ids = [], [], []\n",
        "    for output in outputs:\n",
        "        for detection in output:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5:  # Порог уверенности\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    return boxes, confidences, class_ids\n",
        "\n",
        "# Извлечение кадров из видео\n",
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Сравнение объектов между видео\n",
        "def compare_videos(video_path1, video_path2):\n",
        "    net, output_layers = load_yolo()\n",
        "    frames1 = extract_frames(video_path1)\n",
        "    frames2 = extract_frames(video_path2)\n",
        "\n",
        "    for frame1, frame2 in zip(frames1[:10], frames2[:10]):  # Сравниваем первые 10 кадров\n",
        "        boxes1, _, _ = detect_objects(frame1, net, output_layers)\n",
        "        boxes2, _, _ = detect_objects(frame2, net, output_layers)\n",
        "\n",
        "        # Простой анализ: если количество объектов совпадает\n",
        "        if len(boxes1) == len(boxes2):\n",
        "            print(\"Кадры могут быть похожи.\")\n",
        "        else:\n",
        "            print(\"Кадры разные.\")\n",
        "\n",
        "video_path1 = '/content/4_2.mp4'\n",
        "video_path2 = '/content/5.mp4'\n",
        "compare_videos(video_path1, video_path2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "LUoFhjno14eM",
        "outputId": "3ee18edf-e7da-4156-f34a-872997249cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Скачивание yolov3.cfg...\n",
            "yolov3.cfg скачан.\n",
            "Скачивание yolov3.weights...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 500: Internal Server Error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-51a9d78abe4f>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mvideo_path1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/4_2.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mvideo_path2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/5.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mcompare_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-51a9d78abe4f>\u001b[0m in \u001b[0;36mcompare_videos\u001b[0;34m(video_path1, video_path2)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Сравнение объектов между видео\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompare_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mframes1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mframes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-51a9d78abe4f>\u001b[0m in \u001b[0;36mload_yolo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Загрузка YOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdownload_yolo_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov3.weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yolov3.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-51a9d78abe4f>\u001b[0m in \u001b[0;36mdownload_yolo_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{base_url}{cfg_file}?raw=true\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"https://pjreddie.com/media/files/{weights_file}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Загрузка YOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-51a9d78abe4f>\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(url, filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Скачивание {filename}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{filename} скачан.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 500: Internal Server Error"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Разбивание видео на блоки, и получение суммеированного/среднего кадра и построение ветора по нему, тут надо протестить насколько надо зарезать видео что бы можно было адекватно сравнивать результаты;"
      ],
      "metadata": {
        "id": "ipvcCYrF6Rr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, filename):\n",
        "    if not os.path.isfile(filename):\n",
        "        print(f\"Скачивание {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(f\"{filename} скачан.\")\n",
        "    else:\n",
        "        print(f\"{filename} уже существует.\")\n",
        "\n",
        "def download_yolo_files():\n",
        "    base_url = \"https://github.com/AlexeyAB/darknet/blob/master/cfg/\"\n",
        "    cfg_file = \"yolov3.cfg\"\n",
        "    weights_file = \"yolov3.weights\"\n",
        "\n",
        "    download_file(f\"{base_url}{cfg_file}?raw=true\", cfg_file)\n",
        "    download_file(f\"https://pjreddie.com/media/files/{weights_file}\", weights_file)\n",
        "\n",
        "def load_yolo():\n",
        "    download_yolo_files()\n",
        "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
        "    layer_names = net.getLayerNames()\n",
        "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    return net, output_layers\n",
        "\n",
        "def detect_objects(frame, net, output_layers):\n",
        "    height, width = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outputs = net.forward(output_layers)\n",
        "\n",
        "    boxes, confidences, class_ids = [], [], []\n",
        "    for output in outputs:\n",
        "        for detection in output:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5:\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    return boxes, confidences, class_ids\n",
        "\n",
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def average_frame(block):\n",
        "    return np.mean(block, axis=0).astype(np.uint8)\n",
        "\n",
        "def compare_frames_l2(frame1, frame2):\n",
        "    return cv2.norm(frame1, frame2, cv2.NORM_L2)\n",
        "\n",
        "def compare_frames_ssim(frame1, frame2):\n",
        "    frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "    frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "    score, _ = ssim(frame1_gray, frame2_gray, full=True)\n",
        "    return score\n",
        "\n",
        "def compare_frames_orb(frame1, frame2):\n",
        "    orb = cv2.ORB_create()\n",
        "    kp1, des1 = orb.detectAndCompute(frame1, None)\n",
        "    kp2, des2 = orb.detectAndCompute(frame2, None)\n",
        "\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "    return len(matches)\n",
        "\n",
        "def compare_videos_combined(video_path1, video_path2, block_size=30):\n",
        "    frames1 = extract_frames(video_path1)\n",
        "    frames2 = extract_frames(video_path2)\n",
        "\n",
        "    blocks1 = [frames1[i:i + block_size] for i in range(0, len(frames1), block_size)]\n",
        "    blocks2 = [frames2[i:i + block_size] for i in range(0, len(frames2), block_size)]\n",
        "\n",
        "    l2_scores = []\n",
        "    ssim_scores = []\n",
        "    orb_scores = []\n",
        "\n",
        "    for block1, block2 in zip(blocks1, blocks2):\n",
        "        avg_frame1 = average_frame(block1)\n",
        "        avg_frame2 = average_frame(block2)\n",
        "\n",
        "        l2_score = compare_frames_l2(avg_frame1, avg_frame2)\n",
        "        l2_scores.append(l2_score)\n",
        "\n",
        "        ssim_score = compare_frames_ssim(avg_frame1, avg_frame2)\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "        orb_score = compare_frames_orb(avg_frame1, avg_frame2)\n",
        "        orb_scores.append(orb_score)\n",
        "\n",
        "    average_l2 = np.mean(l2_scores)\n",
        "    average_ssim = np.mean(ssim_scores)\n",
        "    total_orb_matches = np.sum(orb_scores)\n",
        "\n",
        "    print(f\"Средняя L2-схожесть между видео: {average_l2}\")\n",
        "    print(f\"Среднее значение SSIM между видео: {average_ssim}\")\n",
        "    print(f\"Общее количество совпадений (ORB): {total_orb_matches}\")\n",
        "\n",
        "video_path1 = '/content/4_2.mp4'\n",
        "video_path2 = '/content/5.mp4'\n",
        "compare_videos_combined(video_path1, video_path2, block_size=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOpaswVt6jYh",
        "outputId": "ff2fcdb1-aa18-4c38-e4af-6e67714d74f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Средняя L2-схожесть между видео: 458.88336560507014\n",
            "Среднее значение SSIM между видео: 0.9976961212912031\n",
            "Общее количество совпадений (ORB): 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Повышение скорости видео с учетом схожести кадров, то есть на быстрой прокрутке соседдние кадры быстро меняются, значит видео можно порезать, то есть выкинуть каждый 2/3/4/5 кадры;"
      ],
      "metadata": {
        "id": "XLUMwPsbnHQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_similar(frame1, frame2, threshold=0.9):\n",
        "    hist1 = cv2.calcHist([frame1], [0, 1], None, [8, 8], [0, 256, 0, 256])\n",
        "    hist2 = cv2.calcHist([frame2], [0, 1], None, [8, 8], [0, 256, 0, 256])\n",
        "    cv2.normalize(hist1, hist1)\n",
        "    cv2.normalize(hist2, hist2)\n",
        "    similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
        "    return similarity > threshold\n",
        "\n",
        "def compare_videos(video1_path, video2_path):\n",
        "    cap1 = cv2.VideoCapture(video1_path)\n",
        "    cap2 = cv2.VideoCapture(video2_path)\n",
        "\n",
        "    while True:\n",
        "        ret1, frame1 = cap1.read()\n",
        "        ret2, frame2 = cap2.read()\n",
        "\n",
        "        if not ret1 or not ret2:\n",
        "            break\n",
        "\n",
        "        if is_similar(frame1, frame2):\n",
        "            print(\"Кадры схожи\")\n",
        "        else:\n",
        "            print(\"Кадры различаются\")\n",
        "\n",
        "    cap1.release()\n",
        "    cap2.release()\n",
        "\n",
        "def process_video(input_video, output_video, skip_frames=3):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    ret, prev_frame = cap.read()\n",
        "    frame_count = 0\n",
        "\n",
        "    while ret:\n",
        "        if frame_count % skip_frames == 0:\n",
        "            out.write(prev_frame)\n",
        "\n",
        "        ret, curr_frame = cap.read()\n",
        "        if ret and is_similar(prev_frame, curr_frame):\n",
        "            frame_count += 1\n",
        "            prev_frame = curr_frame\n",
        "            continue\n",
        "        prev_frame = curr_frame\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "video1 = 'video1.mp4'\n",
        "video2 = 'video2.mp4'\n",
        "compare_videos(video1, video2)\n",
        "process_video(video1, 'output_video.mp4')"
      ],
      "metadata": {
        "id": "RJKbVtRW7EKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Обрезка видео (по вертикали, по горизонтали, по произвольному углу, по центру, по углам и т.д. для снижения объема обрабатываемых данных, то есть не все пиксели надо обрабатывать);"
      ],
      "metadata": {
        "id": "9kM3gJh2n4vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def crop_video(input_video, output_video, crop_region):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, 30.0, (crop_region[2], crop_region[3]))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        cropped_frame = frame[crop_region[1]:crop_region[1]+crop_region[3],\n",
        "                              crop_region[0]:crop_region[0]+crop_region[2]]\n",
        "\n",
        "        out.write(cropped_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "input_video = 'input.mp4'\n",
        "output_video = 'output_cropped.mp4'\n",
        "\n",
        "crop_region = (100, 50, 400, 300)\n",
        "crop_video(input_video, output_video, crop_region)"
      ],
      "metadata": {
        "id": "CayRXKfZoK3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Извлечения ключевых кадров из видео с использованием алгоритма KLT (Kanade-Lucas-Tomasi)"
      ],
      "metadata": {
        "id": "Qob023ajocrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_keyframes(input_video, output_video):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    ret, prev_frame = cap.read()\n",
        "\n",
        "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
        "    lk_params = dict(winSize=(15, 15), maxLevel=2)\n",
        "    p0 = cv2.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, 30.0, (prev_frame.shape[1], prev_frame.shape[0]))\n",
        "\n",
        "    keyframes = [prev_frame]\n",
        "\n",
        "    while True:\n",
        "        ret, curr_frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, p0, None, **lk_params)\n",
        "\n",
        "        good_new = p1[st == 1]\n",
        "        good_old = p0[st == 1]\n",
        "\n",
        "        if np.linalg.norm(good_new - good_old) > 10:\n",
        "            keyframes.append(curr_frame)\n",
        "            out.write(curr_frame)\n",
        "\n",
        "        prev_gray = curr_gray.copy()\n",
        "        p0 = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "input_video = 'input.mp4'\n",
        "output_video = 'output_keyframes.mp4'\n",
        "extract_keyframes(input_video, output_video)"
      ],
      "metadata": {
        "id": "JWZnDAmlop3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. Применение методов нормализации освещения для уменьшения влияния различных условий освещения на сравнение"
      ],
      "metadata": {
        "id": "kBGtoyVGo7K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def normalize_lighting(image):\n",
        "    yuv_image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
        "\n",
        "    y_channel = yuv_image[:, :, 0]\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    cl1 = clahe.apply(y_channel)\n",
        "\n",
        "    yuv_image[:, :, 0] = cl1\n",
        "    normalized_image = cv2.cvtColor(yuv_image, cv2.COLOR_YUV2BGR)\n",
        "\n",
        "    return normalized_image\n",
        "\n",
        "def process_video(input_video, output_video):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        normalized_frame = normalize_lighting(frame)\n",
        "        out.write(normalized_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "input_video = 'input.mp4'\n",
        "output_video = 'output_normalized.mp4'\n",
        "process_video(input_video, output_video)"
      ],
      "metadata": {
        "id": "U4pcOci-pKUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Использование временных меток для определения схожести видео на основе времени записи"
      ],
      "metadata": {
        "id": "T0DMam1Epwjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_timestamp(video_path):\n",
        "    clip = VideoFileClip(video_path)\n",
        "    return clip.reader.fps, clip.reader.duration, clip.reader.start_time\n",
        "\n",
        "def compare_video_timestamps(video1, video2, time_threshold=5):\n",
        "    fps1, duration1, start_time1 = get_video_timestamp(video1)\n",
        "    fps2, duration2, start_time2 = get_video_timestamp(video2)\n",
        "\n",
        "    print(f\"Video 1: Start time: {start_time1}, Duration: {duration1}, FPS: {fps1}\")\n",
        "    print(f\"Video 2: Start time: {start_time2}, Duration: {duration2}, FPS: {fps2}\")\n",
        "\n",
        "    if abs(start_time1 - start_time2) < time_threshold:\n",
        "        print(\"Видео сняты в близкие моменты времени.\")\n",
        "    else:\n",
        "        print(\"Видео сняты в разное время.\")\n",
        "\n",
        "video1 = 'video1.mp4'\n",
        "video2 = 'video2.mp4'\n",
        "compare_video_timestamps(video1, video2)"
      ],
      "metadata": {
        "id": "kghaWBWBpv_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. Использование технологий распознавания лиц для выявления совпадений в видео, что может помочь в поиске дубликатов."
      ],
      "metadata": {
        "id": "T8Jo-UpLswrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(input_video, output_video):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, 30.0,\n",
        "                          (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    known_faces = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "        face_locations = face_recognition.face_locations(rgb_frame)\n",
        "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
        "\n",
        "        for face_encoding in face_encodings:\n",
        "            matches = face_recognition.compare_faces(known_faces, face_encoding)\n",
        "\n",
        "            if True in matches:\n",
        "                print(\"Совпадение найдено!\")\n",
        "            else:\n",
        "                known_faces.append(face_encoding)\n",
        "\n",
        "        for (top, right, bottom, left) in face_locations:\n",
        "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "input_video = 'input_video.mp4'\n",
        "output_video = 'output_video.mp4'\n",
        "process_video(input_video, output_video)"
      ],
      "metadata": {
        "id": "24owrFcTsplQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. Перевод видео в ч/б и выделение только маски"
      ],
      "metadata": {
        "id": "TwvXYF8YtHqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(input_video, output_video):\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, 30.0,\n",
        "                          (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        _, mask = cv2.threshold(gray_frame, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        out.write(cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR))\n",
        "\n",
        "        cv2.imshow('Gray Frame', gray_frame)\n",
        "        cv2.imshow('Mask', mask)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "input_video = 'input_video.mp4'\n",
        "output_video = 'output_mask.mp4'\n",
        "process_video(input_video, output_video)"
      ],
      "metadata": {
        "id": "SOx7n_UItHfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. Temporal Alignment: Методы для временного выравнивания видео и нахождения синхронных частей"
      ],
      "metadata": {
        "id": "7Q6_C8IQ0JFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "from scipy.signal import correlate\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    return np.mean(mfccs, axis=1)  # Возвращаем средние значения MFCC\n",
        "\n",
        "def find_matching_segments(features1, features2):\n",
        "    correlation = correlate(features1, features2)\n",
        "    return np.max(correlation), np.argmax(correlation)\n",
        "\n",
        "def process_video(input_video, audio_path):\n",
        "    audio_features = extract_audio_features(audio_path)\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "    frame_count = 0\n",
        "    matching_frame_indices = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "        if frame_count % 30 == 0:\n",
        "            matching_frame_indices.append(frame_count)\n",
        "\n",
        "        cv2.imshow('Video Frame', frame)\n",
        "\n",
        "        frame_count += 1\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    for index in matching_frame_indices:\n",
        "        print(f'Кадр {index} может быть синхронизирован с аудио.')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_video = 'input_video.mp4'\n",
        "    audio_path = 'audio_file.wav'\n",
        "    process_video(input_video, audio_path)"
      ],
      "metadata": {
        "id": "dJwKCDsr0IOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zTIw2ZtfHOYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VideoFeatureExtractor, self).__init__()\n",
        "        self.cnn = models.resnet50(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_frames, channels, height, width = x.shape\n",
        "        features = []\n",
        "        for i in range(num_frames):\n",
        "            frame = x[:, i, :, :, :]\n",
        "            feature = self.cnn(frame)\n",
        "            features.append(feature)\n",
        "        features = torch.stack(features, dim=1)\n",
        "        return features\n",
        "\n",
        "class KANLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(KANLayer, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = torch.tanh(self.fc1(x))\n",
        "        y = self.fc2(h)\n",
        "        return y\n",
        "\n",
        "class KANVideoComparisonModel(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim, output_dim):\n",
        "        super(KANVideoComparisonModel, self).__init__()\n",
        "        self.feature_extractor = VideoFeatureExtractor()\n",
        "        self.kan_layer = KANLayer(feature_dim, hidden_dim, output_dim)\n",
        "        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, video1, video2):\n",
        "\n",
        "        features1 = self.feature_extractor(video1)\n",
        "        features2 = self.feature_extractor(video2)\n",
        "\n",
        "        kan_rep1 = self.kan_layer(features1.mean(dim=1))\n",
        "        kan_rep2 = self.kan_layer(features2.mean(dim=1))\n",
        "\n",
        "        similarity = self.cosine_similarity(kan_rep1, kan_rep2)\n",
        "        return similarity\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = KANVideoComparisonModel(feature_dim=2048, hidden_dim=512, output_dim=128).to(device)\n",
        "\n",
        "video1 = torch.randn(8, 16, 3, 224, 224).to(device)\n",
        "video2 = torch.randn(8, 16, 3, 224, 224).to(device)\n",
        "\n",
        "similarity = model(video1, video2)\n",
        "print(f\"Схожесть между видео: {similarity.mean().item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "OiMNPxVbHPUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}